{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 기본 모듈 로드 (numpy, pandas)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn 모듈 로드\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# torch 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math # Positional Encoding에 사용\n",
    "\n",
    "# 시드 설정 (재현성 확보)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 데이터 로딩 및 전처리 ###\n",
      "스케일링 완료된 데이터 Shape: (3654, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"### 데이터 로딩 및 전처리 ###\")\n",
    "# 기초데이터 불러오기\n",
    "# 파일 경로는 실제 파일 위치에 맞게 수정해주세요.\n",
    "try:\n",
    "    X = pd.read_csv('../BASEL_X.csv')\n",
    "    Y = pd.read_csv('../BASEL_Y.csv') # Y는 예측 문제에서는 사용하지 않음\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV 파일을 찾을 수 없습니다. 파일 경로를 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 데이터 시간 순 정렬 (시계열 처리에 필수)\n",
    "X['DATE'] = pd.to_datetime(X['DATE'])\n",
    "X = X.sort_values(by='DATE').reset_index(drop=True)\n",
    "\n",
    "# 1. temp_range (하루 온도 변화량)\n",
    "X['temp_range'] = X['BASEL_temp_max'] - X['BASEL_temp_min']\n",
    "\n",
    "# # 2. humidity_pressure_ratio (습도 대비 기압 비율)\n",
    "epsilon = 1e-6\n",
    "X['humidity_pressure_ratio'] = X['BASEL_humidity'] / (X['BASEL_pressure'] + epsilon)\n",
    "\n",
    "# # 3. cloud_sun_ratio (구름량 대비 햇빛량 비율)\n",
    "X['cloud_sun_ratio'] = X['BASEL_cloud_cover'] / (X['BASEL_sunshine'] + epsilon)\n",
    "\n",
    "# # 4. radiation_precip_ratio (방사선량 대비 강수량 비율)\n",
    "X['radiation_precip_ratio'] = X['BASEL_global_radiation'] / (X['BASEL_precipitation'] + epsilon)\n",
    "feature_columns = [\n",
    "    'BASEL_cloud_cover', 'BASEL_humidity', 'BASEL_pressure',\n",
    "    'BASEL_global_radiation', 'BASEL_precipitation', 'BASEL_sunshine', 'BASEL_temp_mean',\n",
    "    'temp_range', 'humidity_pressure_ratio', 'cloud_sun_ratio', 'radiation_precip_ratio'\n",
    "]\n",
    "\n",
    "\n",
    "target_column = 'BASEL_temp_mean' # 예측할 컬럼 이름\n",
    "input_dim = len(feature_columns) # 입력 특징 차원\n",
    "output_dim = 1 # 예측할 출력 특징 차원 (temp_mean 하나)\n",
    "\n",
    "X_pre = X[feature_columns]\n",
    "\n",
    "# 스케일링 (시계열 데이터 전체에 대해 수행)\n",
    "# MinMaxScaler는 0-1 범위로 스케일링하여 신경망 학습에 안정적\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_pre)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_columns) # 스케일링된 데이터로 DataFrame 재생성\n",
    "\n",
    "print(f\"스케일링 완료된 데이터 Shape: {X_scaled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 시퀀스 데이터 생성 ###\n",
      "생성된 입력 시퀀스 Shape: (3648, 4, 11) (num_sequences, window_in, input_dim)\n",
      "생성된 출력 시퀀스 Shape: (3648, 3) (num_sequences, window_out)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### 시퀀스 데이터 생성 ###\")\n",
    "# 시계열 시퀀스 (window) 생성\n",
    "# 이전 window_in 길이로 다음 window_out 길이 예측\n",
    "\n",
    "window_in = 4 # 입력 시퀀스 길이 (이전 4일)\n",
    "window_out = 3 # 예측 시퀀스 길이 (다음 3일)\n",
    "stride = 1 # 시퀀스 생성 시 이동 간격 (데이터 양을 늘리기 위해 1로 설정)\n",
    "\n",
    "X_ts, Y_ts = [], []\n",
    "# 슬라이딩 윈도우 적용\n",
    "# 시퀀스 생성 범위: [start]부터 [start + window_in + window_out - 1]까지\n",
    "# 예측 대상의 마지막 시점 인덱스는 start + window_in + window_out - 1\n",
    "for start in range(0, len(X_scaled_df) - window_in - window_out + 1, stride):\n",
    "    end_input = start + window_in # 입력 시퀀스 끝 인덱스 (포함 안됨)\n",
    "    end_output = end_input + window_out # 출력 시퀀스 끝 인덱스 (포함 안됨)\n",
    "\n",
    "    # 입력 시퀀스: start 부터 end_input-1 까지의 모든 특징\n",
    "    x_seq = X_scaled_df.iloc[start : end_input, :].values\n",
    "    # 출력 시퀀스: end_input 부터 end_output-1 까지의 'temp_mean' 값\n",
    "    # Y_ts는 (num_sequences, window_out) 형태가 되도록 함\n",
    "    y_seq = X_scaled_df.iloc[end_input : end_output, X_scaled_df.columns.get_loc(target_column)].values\n",
    "\n",
    "    X_ts.append(x_seq)\n",
    "    Y_ts.append(y_seq)\n",
    "\n",
    "X_ts_np = np.array(X_ts)\n",
    "Y_ts_np = np.array(Y_ts) # Shape: (num_sequences, window_out)\n",
    "\n",
    "print(f\"생성된 입력 시퀀스 Shape: {X_ts_np.shape} (num_sequences, window_in, input_dim)\")\n",
    "print(f\"생성된 출력 시퀀스 Shape: {Y_ts_np.shape} (num_sequences, window_out)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Train, Validation, Test set 분할 (시간 순서 기준, Test 400개 고정) ###\n",
      "Train set Shape: (1948, 4, 11), (1948, 3)\n",
      "Validation set Shape: (1300, 4, 11), (1300, 3)\n",
      "Test set Shape: (400, 4, 11), (400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Train, Validation, Test set 분할 (시간 순서 기준, Test 400개 고정) ###\")\n",
    "\n",
    "total_sequences = X_ts_np.shape[0]\n",
    "\n",
    "test_size = 400  # 테스트셋은 마지막 400개 고정\n",
    "remaining_size = total_sequences - test_size  # train + validation 크기\n",
    "\n",
    "# train : validation 비율 설정 (나머지 데이터에 대해)\n",
    "train_ratio = 0.6  # 70%를 학습, 30%를 검증\n",
    "\n",
    "train_size = int(remaining_size * train_ratio)\n",
    "val_size = remaining_size - train_size\n",
    "\n",
    "# 데이터 분할\n",
    "X_train = X_ts_np[:train_size]\n",
    "Y_train = Y_ts_np[:train_size]\n",
    "\n",
    "X_val = X_ts_np[train_size:train_size + val_size]\n",
    "Y_val = Y_ts_np[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X_ts_np[-test_size:]  # 마지막 400개\n",
    "Y_test = Y_ts_np[-test_size:]\n",
    "\n",
    "print(f\"Train set Shape: {X_train.shape}, {Y_train.shape}\")\n",
    "print(f\"Validation set Shape: {X_val.shape}, {Y_val.shape}\")\n",
    "print(f\"Test set Shape: {X_test.shape}, {Y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### PyTorch DataLoader 준비 ###\n",
      "Train_Loader 배치 개수: 16\n",
      "Val_Loader 배치 개수: 11\n",
      "Test_Loader 배치 개수: 4\n",
      "\n",
      "### Seq2Seq (Encoder-Decoder) 모델 정의 및 학습 ###\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### PyTorch DataLoader 준비 ###\")\n",
    "# TensorDataset 및 DataLoader 생성\n",
    "D_train = TensorDataset(torch.Tensor(X_train).to(device), torch.Tensor(Y_train).to(device))\n",
    "D_val = TensorDataset(torch.Tensor(X_val).to(device), torch.Tensor(Y_val).to(device)) # 검증 데이터로더 추가\n",
    "D_test = TensorDataset(torch.Tensor(X_test).to(device), torch.Tensor(Y_test).to(device))\n",
    "\n",
    "Train_Loader = DataLoader(D_train, batch_size=128, shuffle=True, drop_last=False) # 학습 시 셔플\n",
    "Val_Loader = DataLoader(D_val, batch_size=128, shuffle=False, drop_last=False) # 검증 시 셔플 안 함\n",
    "Test_Loader = DataLoader(D_test, batch_size=128, shuffle=False, drop_last=False) # 테스트 시 셔플 안 함\n",
    "\n",
    "print(f\"Train_Loader 배치 개수: {len(Train_Loader)}\")\n",
    "print(f\"Val_Loader 배치 개수: {len(Val_Loader)}\")\n",
    "print(f\"Test_Loader 배치 개수: {len(Test_Loader)}\")\n",
    "\n",
    "\n",
    "#%%\n",
    "print(\"\\n### Seq2Seq (Encoder-Decoder) 모델 정의 및 학습 ###\")\n",
    "\n",
    "# 모델 파라미터 (하이퍼파라미터)\n",
    "input_dim = X_train.shape[-1] # 입력 특징 차원\n",
    "hidden_dim = 128 # Hidden 차원 (RNN/LSTM보다 크게 잡는 경향)\n",
    "output_dim = window_out # 예측할 미래 스텝 수 (3일) - 디코더의 최종 출력 차원\n",
    "num_layers = 2 # 레이어 수\n",
    "\n",
    "# --- Encoder 정의 ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM 레이어 정의 (batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "\n",
    "        # LSTM 순전파\n",
    "        # out shape: (batch_size, seq_length, hidden_dim) - 보통 디코더 입력으로 사용 (어텐션 시) 또는 버려짐\n",
    "        # hidden shape: (h_n, c_n) where h_n, c_n shape: (num_layers, batch_size, hidden_dim) - 컨텍스트 벡터로 사용\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # 마지막 스텝의 hidden state, cell state 반환 (컨텍스트 벡터)\n",
    "        return hidden, cell\n",
    "\n",
    "# --- Decoder 정의 (Simplified for Fixed-Length Time Series) ---\n",
    "# 이 디코더는 인코더의 최종 상태를 받아 미래 window_out 값을 한번에 예측합니다.\n",
    "# 전형적인 step-by-step 디코딩 방식과는 다릅니다.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # 인코더의 최종 hidden state를 받아 output_dim으로 매핑하는 선형 레이어\n",
    "        # 인코더의 마지막 레이어 hidden state (hidden[-1, :, :])를 입력으로 받음\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_hidden):\n",
    "        # encoder_hidden shape: (1, batch_size, hidden_dim) if num_layers=1, or\n",
    "        #                      (num_layers, batch_size, hidden_dim) generally\n",
    "        # 우리는 마지막 레이어의 hidden state를 사용 (num_layers 방향의 마지막)\n",
    "        # hidden[-1, :, :] shape: (batch_size, hidden_dim)\n",
    "        prediction = self.fc(encoder_hidden[-1, :, :])\n",
    "\n",
    "        # prediction shape: (batch_size, output_dim)\n",
    "        return prediction\n",
    "\n",
    "# --- Seq2Seq 모델 정의 ---\n",
    "class Seq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = Decoder(hidden_dim, output_dim) # 디코더는 인코더의 hidden_dim을 입력받음\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, window_in, input_dim)\n",
    "\n",
    "        # 인코더를 통과하여 컨텍스트 벡터 (hidden, cell) 얻음\n",
    "        encoder_hidden, encoder_cell = self.encoder(src)\n",
    "\n",
    "        # 디코더는 인코더의 최종 hidden state를 사용하여 예측 시퀀스 생성\n",
    "        # simplified decoder assumes prediction is based only on the final context\n",
    "        prediction = self.decoder(encoder_hidden)\n",
    "\n",
    "        # prediction shape: (batch_size, output_dim) where output_dim = window_out\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "# hidden_dim은 인코더와 디코더에서 일관되게 사용\n",
    "seq2seq_model = Seq2SeqPredictor(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "criterion = nn.MSELoss() # 예측 문제이므로 MSE 사용\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=1e-3) # Adam 옵티마이저\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터\n",
    "num_epochs = 300 # 에폭 수를 늘려볼 수 있음 (Seq2Seq는 더 오래 학습시키는 경우도 많음)\n",
    "# 조기 종료 (Early Stopping)를 위한 변수\n",
    "best_val_loss = float('inf')\n",
    "patience = 30 # 검증 손실 개선이 없을 때 기다릴 에폭 수\n",
    "epochs_no_improve = 0\n",
    "noise_std = 0.01 # 노이즈 추가하여 robutness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq 모델 학습 시작 (300 에폭)...\n",
      "Epoch 1/300: Train Loss = 0.1282, Val Loss = 0.0308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: Train Loss = 0.0076, Val Loss = 0.0079\n",
      "Epoch 20/300: Train Loss = 0.0065, Val Loss = 0.0071\n",
      "Epoch 30/300: Train Loss = 0.0062, Val Loss = 0.0069\n",
      "Epoch 40/300: Train Loss = 0.0062, Val Loss = 0.0070\n",
      "Epoch 50/300: Train Loss = 0.0059, Val Loss = 0.0061\n",
      "Epoch 60/300: Train Loss = 0.0054, Val Loss = 0.0060\n",
      "Epoch 70/300: Train Loss = 0.0054, Val Loss = 0.0059\n",
      "Epoch 80/300: Train Loss = 0.0051, Val Loss = 0.0060\n",
      "Epoch 90/300: Train Loss = 0.0049, Val Loss = 0.0052\n",
      "Epoch 100/300: Train Loss = 0.0049, Val Loss = 0.0055\n",
      "Epoch 110/300: Train Loss = 0.0048, Val Loss = 0.0051\n",
      "Epoch 120/300: Train Loss = 0.0049, Val Loss = 0.0052\n",
      "Epoch 130/300: Train Loss = 0.0048, Val Loss = 0.0051\n",
      "Epoch 140/300: Train Loss = 0.0048, Val Loss = 0.0051\n",
      "Epoch 150/300: Train Loss = 0.0047, Val Loss = 0.0051\n",
      "Epoch 160/300: Train Loss = 0.0048, Val Loss = 0.0055\n",
      "Epoch 170/300: Train Loss = 0.0045, Val Loss = 0.0050\n",
      "Epoch 180/300: Train Loss = 0.0046, Val Loss = 0.0051\n",
      "조기 종료! 검증 손실이 30 에폭 동안 개선되지 않았습니다.\n",
      "Seq2Seq 모델 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seq2Seq 모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "def apply_label_smoothing(y, smoothing=0.05):\n",
    "    return y * (1 - smoothing) + 0.5 * smoothing\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    seq2seq_model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # 학습 데이터 순전파 및 역전파\n",
    "    for batch_x, batch_y in Train_Loader:\n",
    "        # batch_x shape: (batch_size, window_in, input_dim)\n",
    "        # batch_y shape: (batch_size, window_out)\n",
    "        batch_x_noisy = batch_x + torch.randn_like(batch_x) * noise_std\n",
    "        optimizer.zero_grad() # 옵티마이저 초기화\n",
    "        smoothed_batch_y = apply_label_smoothing(batch_y, smoothing=0.05)\n",
    "        prediction = seq2seq_model(batch_x) # 모델 순전파\n",
    "        loss = criterion(prediction, batch_y) # 손실 계산\n",
    "\n",
    "        loss.backward() # 역전파\n",
    "        optimizer.step() # 가중치 업데이트\n",
    "\n",
    "        total_train_loss += loss.item() * batch_x.size(0) # 배치 사이즈 고려 합산\n",
    "\n",
    "    # 학습 데이터 평균 손실 계산\n",
    "    avg_train_loss = total_train_loss / len(Train_Loader.dataset)\n",
    "\n",
    "    # 검증 데이터로 모델 평가\n",
    "    seq2seq_model.eval() # 평가 모드\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "        for batch_x_val, batch_y_val in Val_Loader:\n",
    "            prediction_val = seq2seq_model(batch_x_val)\n",
    "            val_loss = criterion(prediction_val, batch_y_val)\n",
    "            total_val_loss += val_loss.item() * batch_x_val.size(0)\n",
    "\n",
    "    # 검증 데이터 평균 손실 계산\n",
    "    avg_val_loss = total_val_loss / len(Val_Loader.dataset)\n",
    "\n",
    "    # 에폭별 손실 출력\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0: # 10 에폭마다 또는 첫 에폭만 출력\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    # 조기 종료 체크\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # 필요하다면 모델 가중치 저장: torch.save(seq2seq_model.state_dict(), 'best_seq2seq_predictor.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"조기 종료! 검증 손실이 {patience} 에폭 동안 개선되지 않았습니다.\")\n",
    "            break # 학습 루프 중단\n",
    "\n",
    "print(\"Seq2Seq 모델 학습 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 학습된 모델로 테스트셋 예측 및 평가 ###\n",
      "테스트셋 평균 MSE: 0.0048\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### 학습된 모델로 테스트셋 예측 및 평가 ###\")\n",
    "\n",
    "seq2seq_model.eval() # 평가 모드\n",
    "prediction = []\n",
    "ground_truth = []\n",
    "test_losses = [] # 테스트 시퀀스별 MSE 저장\n",
    "\n",
    "with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "    for batch_x, batch_y in Test_Loader:\n",
    "        # batch_x shape: (batch_size, window_in, input_dim)\n",
    "        # batch_y shape: (batch_size, window_out)\n",
    "\n",
    "        output = seq2seq_model(batch_x) # 예측 수행\n",
    "\n",
    "        # 예측 결과와 실제 값 저장\n",
    "        prediction.append(output.cpu().numpy())\n",
    "        ground_truth.append(batch_y.cpu().numpy())\n",
    "\n",
    "        # 시퀀스별 MSE 계산\n",
    "        # criterion(output, batch_y, reduction='none') 사용 시 (batch_size, window_out) 형태 반환\n",
    "        # 각 시퀀스(batch)의 MSE 평균을 구함\n",
    "        batch_mse = torch.mean((output - batch_y)**2, dim=1) # (batch_size,) 형태\n",
    "        test_losses.append(batch_mse.cpu().numpy())\n",
    "\n",
    "\n",
    "prediction = np.concatenate(prediction)\n",
    "ground_truth = np.concatenate(ground_truth)\n",
    "test_losses = np.concatenate(test_losses) # 각 테스트 시퀀스에 대한 MSE 값 배열\n",
    "\n",
    "\n",
    "# 테스트셋 전체 평균 MSE 계산\n",
    "overall_test_mse = np.mean(test_losses)\n",
    "\n",
    "print(f\"테스트셋 평균 MSE: {overall_test_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
